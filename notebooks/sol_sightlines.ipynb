{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Ray tracing with yt + trident for sightlines defined in los_endpoints_obsOriented.csv\n",
    "\n",
    "- Dataset: TNG50-1, snap_099.*.hdf5 chunks\n",
    "- Units for endpoints: absolute ckpc/h (matches Gadget native at z~0)\n",
    "- For each (SubhaloID, span), pair 'minus' and 'plus' endpoints -> one ray\n",
    "- Spectra: raw & COS-G130M LSF for H I 1216 (Lyα), C II 1334, Si III 1206\n",
    "- Column densities computed from number densities:\n",
    "    H_p0, C_p1, Si_p2 (and 'dl' path length from yt)\n",
    "\n",
    "Outputs under: /scratch/tsingh65/TNG50-1_snap99/result/\n",
    "    result/\n",
    "      rays/\n",
    "        sid<SID>/span_<SPAN>/ray_<RID>.h5    <-- spectra + per-cell columns\n",
    "      logs/\n",
    "      summary_rays.csv\n",
    "\n",
    "Usage:\n",
    "    python run_trident_rays.py --mode test \\\n",
    "        --data-dir /scratch/tsingh65/TNG50-1_snap99/data \\\n",
    "        --endpoints /scratch/tsingh65/TNG50-1_snap99/los_endpoints_obsOriented.csv\n",
    "\n",
    "    python run_trident_rays.py --mode all \\\n",
    "        --data-dir /scratch/tsingh65/TNG50-1_snap99/data \\\n",
    "        --endpoints /scratch/tsingh65/TNG50-1_snap99/los_endpoints_obsOriented.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import h5py\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yt\n",
    "import trident\n",
    "\n",
    "# ----------------------------\n",
    "# Config defaults (can be args)\n",
    "# ----------------------------\n",
    "DEFAULT_DATA_DIR   = \"/scratch/tsingh65/TNG50-1_snap99/data\"\n",
    "DEFAULT_ENDPOINTS  = \"/scratch/tsingh65/TNG50-1_snap99/los_endpoints_obsOriented.csv\"\n",
    "DEFAULT_OUT_ROOT   = \"/scratch/tsingh65/TNG50-1_snap99/result\"\n",
    "FIRST_CHUNK        = \"snap_099.0.hdf5\"\n",
    "INDEX_FILENAME     = \"snap_099.0.hdf5.ewah\"   # per your note (yt index cache)\n",
    "\n",
    "IONS     = [\"H I 1216\", \"C II 1334\", \"Si III 1206\"]\n",
    "ION_DENS = {\n",
    "    \"H I 1216\": (\"H_p0_number_density\", \"H I\"),\n",
    "    \"C II 1334\": (\"C_p1_number_density\", \"C II\"),\n",
    "    \"Si III 1206\": (\"Si_p2_number_density\", \"Si III\"),\n",
    "}\n",
    "\n",
    "LSF_NAME = \"COS-G130M\"\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def load_dataset(data_dir: str):\n",
    "    \"\"\"\n",
    "    Load Gadget snapshot by pointing yt at the first chunk.\n",
    "    Also pass/attach the .ewah index filename if supported.\n",
    "    \"\"\"\n",
    "    first = os.path.join(data_dir, FIRST_CHUNK)\n",
    "    if not os.path.isfile(first):\n",
    "        raise FileNotFoundError(f\"Cannot find first chunk: {first}\")\n",
    "\n",
    "    idx_path = os.path.join(data_dir, INDEX_FILENAME)\n",
    "    try:\n",
    "        ds = yt.load(first, index_filename=idx_path)\n",
    "    except TypeError:\n",
    "        # yt may not accept index_filename kwarg; load then set attribute if present\n",
    "        ds = yt.load(first)\n",
    "        if hasattr(ds, \"index_filename\"):\n",
    "            ds.index_filename = idx_path\n",
    "    return ds\n",
    "\n",
    "def read_and_pair_endpoints(csv_path: str):\n",
    "    \"\"\"\n",
    "    Read los_endpoints_obsOriented.csv and pair the +/- points into\n",
    "    rays per (SubhaloID, span).\n",
    "\n",
    "    Returns a DataFrame with one row per ray:\n",
    "      columns: Galaxy, SubhaloID, span, inc_deg, phi_deg, rho_kpc, Rvir_kpc,\n",
    "               start_ckpch(3), end_ckpch(3), ray_id\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # strictly require these to exist\n",
    "    req = [\"Galaxy\",\"SubhaloID\",\"span\",\"sign\",\"X_ckpch\",\"Y_ckpch\",\"Z_ckpch\",\n",
    "           \"inc_deg\",\"phi_deg\",\"rho_kpc\",\"Rvir_kpc\"]\n",
    "    missing = [c for c in req if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns in {csv_path}: {missing}\")\n",
    "\n",
    "    # Pair by key = (SubhaloID, span). If multiple galaxies (unlikely), keep Galaxy from first.\n",
    "    grp_cols = [\"SubhaloID\", \"span\"]\n",
    "    rays = []\n",
    "    for (sid, span), g in df.groupby(grp_cols):\n",
    "        # Expect exactly one 'minus' and one 'plus'\n",
    "        if set(g[\"sign\"].unique()) != {\"minus\", \"plus\"}:\n",
    "            # Skip incomplete pairs (quietly), but record a warning\n",
    "            print(f\"[WARN] Incomplete +/- pair for sid={sid}, span={span}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        g_minus = g[g[\"sign\"]==\"minus\"].iloc[0]\n",
    "        g_plus  = g[g[\"sign\"]==\"plus\"].iloc[0]\n",
    "\n",
    "        # Build ray row\n",
    "        galaxy = str(g_minus[\"Galaxy\"])\n",
    "        inc    = float(g_minus[\"inc_deg\"])\n",
    "        phi    = float(g_minus[\"phi_deg\"])\n",
    "        rho    = float(g_minus[\"rho_kpc\"])\n",
    "        rvir   = float(g_minus[\"Rvir_kpc\"])\n",
    "\n",
    "        start = np.array([g_minus[\"X_ckpch\"], g_minus[\"Y_ckpch\"], g_minus[\"Z_ckpch\"]], dtype=float)\n",
    "        end   = np.array([g_plus[\"X_ckpch\"],  g_plus[\"Y_ckpch\"],  g_plus[\"Z_ckpch\"] ], dtype=float)\n",
    "\n",
    "        ray_id = f\"sid{int(sid)}_{span}\"\n",
    "\n",
    "        rays.append(dict(\n",
    "            Galaxy=galaxy,\n",
    "            SubhaloID=int(sid),\n",
    "            span=span,\n",
    "            inc_deg=inc,\n",
    "            phi_deg=phi,\n",
    "            rho_kpc=rho,\n",
    "            Rvir_kpc=rvir,\n",
    "            start_ckpch=start,\n",
    "            end_ckpch=end,\n",
    "            ray_id=ray_id\n",
    "        ))\n",
    "\n",
    "    if not rays:\n",
    "        raise RuntimeError(\"No valid +/- endpoint pairs found. Check your CSV.\")\n",
    "    return pd.DataFrame(rays)\n",
    "\n",
    "def make_ray(ds, start_ckpch, end_ckpch, fields=None):\n",
    "    \"\"\"\n",
    "    Build a single ray with trident.make_simple_ray using absolute ckpc/h endpoints.\n",
    "    Returns the yt Dataset 'ray' object (not saved to file).\n",
    "    \"\"\"\n",
    "    sp = yt.YTArray(start_ckpch, \"kpc/h\")\n",
    "    ep = yt.YTArray(end_ckpch,   \"kpc/h\")\n",
    "\n",
    "    print(\"Making ray from\", sp, \"to\", ep)\n",
    "    \n",
    "\n",
    "    # If ion fields are *already* in the dataset, you can skip add_ion_fields.\n",
    "    # If you needed them: trident.add_ion_fields(ds, ions=['H I','C II','Si III'])\n",
    "\n",
    "    ray = trident.make_simple_ray(\n",
    "        ds,\n",
    "        start_position=sp,\n",
    "        end_position=ep,\n",
    "        fields=(fields or [\n",
    "            (\"gas\", \"density\"),\n",
    "            (\"gas\", \"temperature\"),\n",
    "            (\"gas\", \"metallicity\"),\n",
    "            (\"gas\", \"dl\"),  # path length per cell\n",
    "            (\"gas\", \"H_p0_number_density\"),\n",
    "            (\"gas\", \"C_p1_number_density\"),\n",
    "            (\"gas\", \"Si_p2_number_density\"),\n",
    "            # add more fields if you need to debug\n",
    "        ]),\n",
    "        ftype=\"gas\",\n",
    "        data_filename=None,   # don't dump a separate ray file (we'll pack into our HDF5)\n",
    "        pix_frac=1.0\n",
    "    )\n",
    "    return ray\n",
    "\n",
    "def compute_columns(ray):\n",
    "    \"\"\"\n",
    "    Column densities (true integrals): sum(n_ion * dl).\n",
    "    Returns dict with totals and per-cell arrays (for optional debugging).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    dl = ray.r[(\"gas\", \"dl\")]  # length in cm by default (yt units)\n",
    "    for disp, (fname, _label) in ION_DENS.items():\n",
    "        n = ray.r[(\"gas\", fname)]\n",
    "        col = (n * dl).sum()   # units: cm^-2\n",
    "        out[f\"N_tot_{disp}\"] = float(col)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def build_spectrum(ray):\n",
    "    \"\"\"\n",
    "    Use instrument preset 'COS-G130M' to let Trident define the wavelength grid\n",
    "    and apply the LSF to the flux. We also save a 'raw' (pre-LSF) flux by\n",
    "    computing exp(-tau) from the returned optical depth.\n",
    "    \"\"\"\n",
    "    line_list = [\"H I 1216\", \"C II 1334\", \"Si III 1206\"]\n",
    "\n",
    "    # Instrument-configured spectrum (includes COS-G130M LSF)\n",
    "    sg = trident.SpectrumGenerator(LSF_NAME)\n",
    "    sg.add_line_list(line_list)\n",
    "    # Alternatively: sg.make_spectrum(ray, lines=line_list) works too\n",
    "    sg.make_spectrum(ray, lines=line_list)\n",
    "\n",
    "    lam = np.array(sg.lambda_field)   # Å\n",
    "    tau = np.array(sg.tau_field)      # intrinsic optical depth on this grid\n",
    "    flux_lsf = np.array(sg.flux_field)  # LSF-convolved flux\n",
    "\n",
    "    # Raw (pre-LSF) flux on the same wavelength grid\n",
    "    flux_raw = np.exp(-tau)\n",
    "\n",
    "    return {\n",
    "        \"raw\": {\"lambda\": lam, \"flux\": flux_raw, \"tau\": tau},\n",
    "        \"lsf\": {\"lambda\": lam, \"flux\": flux_lsf, \"tau\": tau},\n",
    "    }\n",
    "    \n",
    "    \n",
    "def save_ray_hdf5(out_path, meta, ray, spec, col_dict):\n",
    "    \"\"\"\n",
    "    Write a single HDF5 containing:\n",
    "      /meta        : JSON-ish attrs (galaxy, sid, span, inc, phi, rho, Rvir)\n",
    "      /spectrum/raw: lambda, flux, tau\n",
    "      /spectrum/lsf: lambda, flux, tau\n",
    "      /columns     : totals (N_tot_HI, N_tot_CII, N_tot_SiIII)\n",
    "    \"\"\"\n",
    "    ensure_dir(os.path.dirname(out_path))\n",
    "    with h5py.File(out_path, \"w\") as f:\n",
    "        # Meta\n",
    "        gmeta = f.create_group(\"meta\")\n",
    "        for k, v in meta.items():\n",
    "            # store small scalars/strings as attrs\n",
    "            try:\n",
    "                gmeta.attrs[k] = v\n",
    "            except TypeError:\n",
    "                gmeta.attrs[k] = json.dumps(v)\n",
    "\n",
    "        # Spectra\n",
    "        gs = f.create_group(\"spectrum\")\n",
    "        for tag in (\"raw\", \"lsf\"):\n",
    "            gt = gs.create_group(tag)\n",
    "            gt.create_dataset(\"lambda_A\", data=spec[tag][\"lambda\"])\n",
    "            gt.create_dataset(\"flux\",     data=spec[tag][\"flux\"])\n",
    "            gt.create_dataset(\"tau\",      data=spec[tag][\"tau\"])\n",
    "\n",
    "        # Columns (totals)\n",
    "        gc = f.create_group(\"columns\")\n",
    "        for k, v in col_dict.items():\n",
    "            gc.attrs[k] = v\n",
    "\n",
    "def process_one_ray(ds, row, out_root):\n",
    "    \"\"\"\n",
    "    Build & save one ray for a paired row (start/end already paired).\n",
    "    Returns path + summary dict.\n",
    "    \"\"\"\n",
    "    sid   = int(row[\"SubhaloID\"])\n",
    "    span  = str(row[\"span\"])\n",
    "    rid   = str(row[\"ray_id\"])\n",
    "    gal   = str(row[\"Galaxy\"])\n",
    "    inc   = float(row[\"inc_deg\"])\n",
    "    phi   = float(row[\"phi_deg\"])\n",
    "    rho   = float(row[\"rho_kpc\"])\n",
    "    rvir  = float(row[\"Rvir_kpc\"])\n",
    "\n",
    "    start = np.array(row[\"start_ckpch\"], dtype=float)\n",
    "    end   = np.array(row[\"end_ckpch\"],   dtype=float)\n",
    "\n",
    "    ray = make_ray(ds, start, end)\n",
    "    spec = build_spectrum(ray)\n",
    "    cols = compute_columns(ray)\n",
    "\n",
    "    out_dir = os.path.join(out_root, \"rays\", f\"sid{sid}\", f\"span_{span}\")\n",
    "    out_h5  = os.path.join(out_dir, f\"ray_{rid}.h5\")\n",
    "\n",
    "    meta = dict(\n",
    "        Galaxy=gal,\n",
    "        SubhaloID=sid,\n",
    "        span=span,\n",
    "        ray_id=rid,\n",
    "        inc_deg=inc, phi_deg=phi, rho_kpc=rho, Rvir_kpc=rvir,\n",
    "        start_ckpch=start.tolist(),\n",
    "        end_ckpch=end.tolist(),\n",
    "        instrument=LSF_NAME,\n",
    "        lambda_min=LAMBDA_MIN, lambda_max=LAMBDA_MAX, dlambda=DLAMBDA,\n",
    "        ions=IONS,\n",
    "    )\n",
    "\n",
    "    save_ray_hdf5(out_h5, meta, ray, spec, cols)\n",
    "\n",
    "    # Also return a summary row for the master CSV\n",
    "    srow = dict(\n",
    "        ray_id=rid,\n",
    "        Galaxy=gal,\n",
    "        SubhaloID=sid,\n",
    "        span=span,\n",
    "        inc_deg=inc, phi_deg=phi, rho_kpc=rho, Rvir_kpc=rvir,\n",
    "        out_h5=out_h5,\n",
    "        N_tot_HI=cols.get(\"N_tot_H I 1216\", np.nan) if \"N_tot_H I 1216\" in cols else cols.get(\"N_tot_H I\", np.nan),\n",
    "        N_tot_CII=cols.get(\"N_tot_C II 1334\", np.nan) if \"N_tot_C II 1334\" in cols else cols.get(\"N_tot_C II\", np.nan),\n",
    "        N_tot_SiIII=cols.get(\"N_tot_Si III 1206\", np.nan) if \"N_tot_Si III 1206\" in cols else cols.get(\"N_tot_Si III\", np.nan),\n",
    "    )\n",
    "    # Fix column keys (because we used display names in compute_columns)\n",
    "    if \"N_tot_H I 1216\" in cols:   srow[\"N_tot_HI\"]    = cols[\"N_tot_H I 1216\"]\n",
    "    if \"N_tot_C II 1334\" in cols:  srow[\"N_tot_CII\"]   = cols[\"N_tot_C II 1334\"]\n",
    "    if \"N_tot_Si III 1206\" in cols:srow[\"N_tot_SiIII\"] = cols[\"N_tot_Si III 1206\"]\n",
    "\n",
    "    return out_h5, srow\n",
    "\n",
    "# ----------------------------\n",
    "# Single-ray \"test\" runner\n",
    "# ----------------------------\n",
    "def run_test(data_dir, endpoints_csv, out_root):\n",
    "    print(\"[TEST] Loading dataset…\")\n",
    "    ds = load_dataset(data_dir)\n",
    "\n",
    "    print(\"[TEST] Reading and pairing endpoints…\")\n",
    "    rays_df = read_and_pair_endpoints(endpoints_csv)\n",
    "    print(f\"[TEST] Paired rays: {len(rays_df)}\")\n",
    "\n",
    "    # Pick the first one as a smoke test\n",
    "    row = rays_df.iloc[0]\n",
    "    print(f\"[TEST] Running 1st ray: ray_id={row['ray_id']} sid={row['SubhaloID']} span={row['span']}\")\n",
    "\n",
    "    out_h5, srow = process_one_ray(ds, row, out_root)\n",
    "    print(f\"[TEST OK] wrote: {out_h5}\")\n",
    "    # Write a tiny test summary\n",
    "    test_csv = os.path.join(out_root, \"summary_rays_TEST.csv\")\n",
    "    pd.DataFrame([srow]).to_csv(test_csv, index=False)\n",
    "    print(f\"[TEST OK] wrote: {test_csv}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Batch runner\n",
    "# ----------------------------\n",
    "def run_all(data_dir, endpoints_csv, out_root):\n",
    "    logs_dir = ensure_dir(os.path.join(out_root, \"logs\"))\n",
    "    print(\"[ALL] Loading dataset…\")\n",
    "    ds = load_dataset(data_dir)\n",
    "\n",
    "    print(\"[ALL] Reading and pairing endpoints…\")\n",
    "    rays_df = read_and_pair_endpoints(endpoints_csv)\n",
    "    print(f\"[ALL] Paired rays: {len(rays_df)}\")\n",
    "\n",
    "    summary_rows = []\n",
    "    for i, row in rays_df.iterrows():\n",
    "        try:\n",
    "            print(f\"[{i+1}/{len(rays_df)}] ray_id={row['ray_id']} sid={row['SubhaloID']} span={row['span']}\")\n",
    "            out_h5, srow = process_one_ray(ds, row, out_root)\n",
    "            summary_rows.append(srow)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] ray failed: ray_id={row.get('ray_id')} sid={row.get('SubhaloID')} span={row.get('span')}: {e}\")\n",
    "            # log error\n",
    "            with open(os.path.join(logs_dir, \"errors.txt\"), \"a\") as f:\n",
    "                f.write(f\"{row.get('ray_id')} | sid={row.get('SubhaloID')} span={row.get('span')} | {repr(e)}\\n\")\n",
    "\n",
    "    # Save master summary\n",
    "    if summary_rows:\n",
    "        master_csv = os.path.join(out_root, \"summary_rays.csv\")\n",
    "        pd.DataFrame(summary_rows).to_csv(master_csv, index=False)\n",
    "        print(f\"[ALL OK] wrote: {master_csv}\")\n",
    "    else:\n",
    "        print(\"[ALL] No successful rays to summarize.\")\n",
    "\n",
    "# ----------------------------\n",
    "# CLI\n",
    "# ----------------------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--mode\", choices=[\"test\",\"all\"], required=True,\n",
    "                    help=\"test: run one ray; all: run all rays\")\n",
    "    ap.add_argument(\"--data-dir\", default=DEFAULT_DATA_DIR,\n",
    "                    help=\"Directory containing snap_099.*.hdf5 chunks\")\n",
    "    ap.add_argument(\"--endpoints\", default=DEFAULT_ENDPOINTS,\n",
    "                    help=\"Path to los_endpoints_obsOriented.csv\")\n",
    "    ap.add_argument(\"--out-root\", default=DEFAULT_OUT_ROOT,\n",
    "                    help=\"Where to save results\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    ensure_dir(args.out_root)\n",
    "    if args.mode == \"test\":\n",
    "        run_test(args.data_dir, args.endpoints, args.out_root)\n",
    "    else:\n",
    "        run_all(args.data_dir, args.endpoints, args.out_root)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
