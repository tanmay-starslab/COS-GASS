{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP1] Reading catalog: ../data/COS_GASS_TNG.txt\n",
      "[STEP1] Catalog rows loaded: 18\n",
      "[STEP1] Galaxies with numeric 'inc': 13\n",
      "[STEP1] Galaxies skipped (inc == '-'): 5\n",
      "[STEP1] Unique SubhaloID to run: 52\n",
      "\n",
      "[STEP1] Outputs written:\n",
      "  • SIDs list:          ../data/sids_with_inc.txt\n",
      "  • Detailed include:   ../data/sids_with_inc_detailed.csv\n",
      "  • Skipped (inc='-'):  ../data/sids_skipped_by_inc.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "STEP 1: Filter subhalos by availability of OBSERVED galaxy inclination in the catalog.\n",
    "\n",
    "- Reads:  /Users/tsingh65/github_repos/COS-GASS/data/COS_GASS_TNG.txt\n",
    "- Keeps only rows where 'inc' is numeric (skips '-')\n",
    "- Explodes SubhaloID1..4 -> unique SubhaloID list to run\n",
    "- Writes:\n",
    "    /Users/tsingh65/github_repos/COS-GASS/data/sids_with_inc.txt         (one SID per line)\n",
    "    /Users/tsingh65/github_repos/COS-GASS/data/sids_with_inc_detailed.csv (mapping per galaxy)\n",
    "    /Users/tsingh65/github_repos/COS-GASS/data/sids_skipped_by_inc.csv    (galaxies with '-' in inc)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CATALOG_CSV = r\"../data/COS_GASS_TNG_.txt\"\n",
    "REPO_DATA   = os.path.dirname(CATALOG_CSV)\n",
    "\n",
    "OUT_TXT     = os.path.join(REPO_DATA, \"sids_with_inc.txt\")\n",
    "OUT_DETAIL  = os.path.join(REPO_DATA, \"sids_with_inc_detailed.csv\")\n",
    "OUT_SKIPPED = os.path.join(REPO_DATA, \"sids_skipped_by_inc.csv\")\n",
    "\n",
    "def main():\n",
    "    print(\"[STEP1] Reading catalog:\", CATALOG_CSV)\n",
    "    if not os.path.isfile(CATALOG_CSV):\n",
    "        raise FileNotFoundError(f\"Catalog not found: {CATALOG_CSV}\")\n",
    "\n",
    "    # Read; treat '-' as NaN for *all* columns to be safe, then force numeric on inc\n",
    "    df = pd.read_csv(CATALOG_CSV, na_values=[\"-\"])\n",
    "    original_rows = len(df)\n",
    "    print(f\"[STEP1] Catalog rows loaded: {original_rows}\")\n",
    "\n",
    "    # Split by availability of numeric inc\n",
    "    df[\"inc\"] = pd.to_numeric(df[\"inc\"], errors=\"coerce\")\n",
    "    has_inc = df[\"inc\"].notna()\n",
    "    df_keep = df.loc[has_inc].copy()\n",
    "    df_skip = df.loc[~has_inc].copy()\n",
    "\n",
    "    print(f\"[STEP1] Galaxies with numeric 'inc': {len(df_keep)}\")\n",
    "    print(f\"[STEP1] Galaxies skipped (inc == '-'): {len(df_skip)}\")\n",
    "\n",
    "    # Explode SubhaloID1..4 to one SID per row\n",
    "    sid_cols = [\"SubhaloID1\",\"SubhaloID2\",\"SubhaloID3\",\"SubhaloID4\"]\n",
    "    for c in sid_cols:\n",
    "        if c not in df_keep.columns:\n",
    "            df_keep[c] = np.nan\n",
    "\n",
    "    # Build long list of (Galaxy, COS_ID, inc, SID)\n",
    "    rows = []\n",
    "    for _, r in df_keep.iterrows():\n",
    "        base = dict(COS_ID=r[\"COS_ID\"], Galaxy=r[\"Galaxy\"], inc=float(r[\"inc\"]))\n",
    "        for c in sid_cols:\n",
    "            sid = r.get(c)\n",
    "            if pd.notna(sid):\n",
    "                try:\n",
    "                    rows.append({**base, \"SubhaloID\": int(sid)})\n",
    "                except Exception:\n",
    "                    pass\n",
    "    dfl = pd.DataFrame(rows).drop_duplicates(subset=[\"SubhaloID\"]).sort_values(\"SubhaloID\")\n",
    "    print(f\"[STEP1] Unique SubhaloID to run: {len(dfl)}\")\n",
    "\n",
    "    # Save artifacts\n",
    "    os.makedirs(REPO_DATA, exist_ok=True)\n",
    "    dfl.to_csv(OUT_DETAIL, index=False)\n",
    "    with open(OUT_TXT, \"w\") as f:\n",
    "        for sid in dfl[\"SubhaloID\"].tolist():\n",
    "            f.write(f\"{sid}\\n\")\n",
    "    df_skip.to_csv(OUT_SKIPPED, index=False)\n",
    "\n",
    "    # Quick debugging summary\n",
    "    print(\"\\n[STEP1] Outputs written:\")\n",
    "    print(\"  • SIDs list:         \", OUT_TXT)\n",
    "    print(\"  • Detailed include:  \", OUT_DETAIL)\n",
    "    print(\"  • Skipped (inc='-'): \", OUT_SKIPPED)\n",
    "    if len(dfl) == 0:\n",
    "        print(\"[STEP1][WARN] No subhalos selected. Did your catalog have numeric 'inc' values?\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522f8249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN2] Loading previously skipped galaxy list: ../data/sids_skipped_by_inc.csv\n",
      "[RUN2] Loading updated 'Complete' catalog: ../data/COS_GASS_TNG_Complete.txt\n",
      "[RUN2] Previously skipped galaxies: 5\n",
      "[RUN2] Rows in 'Complete' matching skipped COS_IDs: 5\n",
      "[RUN2] Matching rows with numeric 'inc' now available: 5\n",
      "[RUN2] Unique SubhaloID to run (run2): 20\n",
      "\n",
      "[RUN2] Outputs written:\n",
      "  • SIDs list (run2):               ../data/sids_with_inc_run2.txt\n",
      "  • Detailed include (run2):        ../data/sids_with_inc_detailed_run2.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "STEP 1 (run2): Build SID list **only** for galaxies that were previously skipped\n",
    "because of missing inclination, using the updated 'Complete' catalog that now\n",
    "includes those inclinations.\n",
    "\n",
    "Inputs (relative to this script):\n",
    "  - ../data/sids_skipped_by_inc.csv            (from run1; the galaxies to revisit)\n",
    "  - ../data/COS_GASS_TNG_Complete.txt          (new catalog with inclinations filled)\n",
    "\n",
    "Outputs:\n",
    "  - ../data/sids_with_inc_run2.txt             (one unique SubhaloID per line)\n",
    "  - ../data/sids_with_inc_detailed_run2.csv    (per-galaxy mapping with COS_ID, Galaxy, inc, SubhaloID)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "REPO_DATA        = r\"../data\"\n",
    "SKIPPED_PREV_CSV = os.path.join(REPO_DATA, \"sids_skipped_by_inc.csv\")\n",
    "CATALOG_COMPLETE = os.path.join(REPO_DATA, \"COS_GASS_TNG_Complete.txt\")\n",
    "\n",
    "OUT_TXT2    = os.path.join(REPO_DATA, \"sids_with_inc_run2.txt\")\n",
    "OUT_DETAIL2 = os.path.join(REPO_DATA, \"sids_with_inc_detailed_run2.csv\")\n",
    "\n",
    "\n",
    "def _coerce_int_series(s):\n",
    "    \"\"\"Safely coerce a Series to integer where possible; returns object dtype.\"\"\"\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    # Keep as object so we can drop NaNs and cast to Python int later where valid\n",
    "    return x.astype(\"Int64\").astype(object)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"[RUN2] Loading previously skipped galaxy list:\", SKIPPED_PREV_CSV)\n",
    "    if not os.path.isfile(SKIPPED_PREV_CSV):\n",
    "        raise FileNotFoundError(f\"Missing: {SKIPPED_PREV_CSV}\")\n",
    "\n",
    "    print(\"[RUN2] Loading updated 'Complete' catalog:\", CATALOG_COMPLETE)\n",
    "    if not os.path.isfile(CATALOG_COMPLETE):\n",
    "        raise FileNotFoundError(f\"Missing: {CATALOG_COMPLETE}\")\n",
    "\n",
    "    # Load the \"skipped\" set from run1 (these are the only galaxies we consider now)\n",
    "    skipped = pd.read_csv(SKIPPED_PREV_CSV)\n",
    "    if \"COS_ID\" not in skipped.columns:\n",
    "        raise KeyError(\"Expected 'COS_ID' column in sids_skipped_by_inc.csv\")\n",
    "    # Canonicalize COS_ID to int-like for a clean join key\n",
    "    skipped[\"COS_ID\"] = pd.to_numeric(skipped[\"COS_ID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    skipped_ids = set([int(v) for v in skipped[\"COS_ID\"].dropna().tolist()])\n",
    "\n",
    "    print(f\"[RUN2] Previously skipped galaxies: {len(skipped_ids)}\")\n",
    "\n",
    "    # Load the new complete catalog; treat '-' and empty as NaN across the board\n",
    "    complete = pd.read_csv(CATALOG_COMPLETE, na_values=[\"-\", \"\"])\n",
    "    if \"COS_ID\" not in complete.columns:\n",
    "        raise KeyError(\"Expected 'COS_ID' column in COS_GASS_TNG_Complete.txt\")\n",
    "\n",
    "    # Filter complete catalog to the COS_IDs that were previously skipped\n",
    "    complete[\"COS_ID\"] = pd.to_numeric(complete[\"COS_ID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = complete[complete[\"COS_ID\"].isin(skipped_ids)].copy()\n",
    "    print(f\"[RUN2] Rows in 'Complete' matching skipped COS_IDs: {len(df)}\")\n",
    "\n",
    "    # Force numeric inclination and keep only those now available\n",
    "    if \"inc\" not in df.columns:\n",
    "        raise KeyError(\"Expected 'inc' column in COS_GASS_TNG_Complete.txt\")\n",
    "\n",
    "    df[\"inc\"] = pd.to_numeric(df[\"inc\"], errors=\"coerce\")\n",
    "    df_keep = df[df[\"inc\"].notna()].copy()\n",
    "    print(f\"[RUN2] Matching rows with numeric 'inc' now available: {len(df_keep)}\")\n",
    "\n",
    "    # Prepare SID columns; ensure presence and numeric coercion\n",
    "    sid_cols = [\"SubhaloID1\", \"SubhaloID2\", \"SubhaloID3\", \"SubhaloID4\"]\n",
    "    for c in sid_cols:\n",
    "        if c not in df_keep.columns:\n",
    "            df_keep[c] = np.nan\n",
    "        df_keep[c] = pd.to_numeric(df_keep[c], errors=\"coerce\")\n",
    "\n",
    "    # Build long format: one SID per row, tagged with galaxy metadata\n",
    "    rows = []\n",
    "    for _, r in df_keep.iterrows():\n",
    "        base = {\n",
    "            \"COS_ID\": int(r[\"COS_ID\"]) if pd.notna(r[\"COS_ID\"]) else None,\n",
    "            \"Galaxy\": r[\"Galaxy\"] if \"Galaxy\" in df_keep.columns else None,\n",
    "            \"inc\": float(r[\"inc\"]),\n",
    "        }\n",
    "        for c in sid_cols:\n",
    "            sid = r[c]\n",
    "            if pd.notna(sid):\n",
    "                sid_int = int(sid)\n",
    "                rows.append({**base, \"SubhaloID\": sid_int})\n",
    "\n",
    "    dfl = pd.DataFrame(rows)\n",
    "    if not dfl.empty:\n",
    "        dfl = dfl.drop_duplicates(subset=[\"SubhaloID\"]).sort_values(\"SubhaloID\")\n",
    "    print(f\"[RUN2] Unique SubhaloID to run (run2): {len(dfl)}\")\n",
    "\n",
    "    # Write outputs\n",
    "    os.makedirs(REPO_DATA, exist_ok=True)\n",
    "\n",
    "    # Detailed CSV\n",
    "    dfl.to_csv(OUT_DETAIL2, index=False)\n",
    "\n",
    "    # Plain-text list\n",
    "    with open(OUT_TXT2, \"w\") as f:\n",
    "        for sid in dfl[\"SubhaloID\"].tolist():\n",
    "            f.write(f\"{sid}\\n\")\n",
    "\n",
    "    print(\"\\n[RUN2] Outputs written:\")\n",
    "    print(\"  • SIDs list (run2):              \", OUT_TXT2)\n",
    "    print(\"  • Detailed include (run2):       \", OUT_DETAIL2)\n",
    "\n",
    "    if len(dfl) == 0:\n",
    "        print(\"[RUN2][WARN] No subhalos selected. Either the 'Complete' catalog still lacks \"\n",
    "              \"inclinations for these COS_IDs or SubhaloID1..4 are all missing.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "STEP 2: Iterate SIDs and call the unmodified single-subhalo script for each.\n",
    "\n",
    "PREREQS:\n",
    "- Run step1_select_sids_with_inc.py first; it writes sids_with_inc.txt\n",
    "- Make sure your LOS generator (the orientations/LOS endpoints code) has\n",
    "  already produced the per-SID rays CSVs in each subhalo directory:\n",
    "    sub_<SID>/rays_and_recipes_sid<SID>_snap99_L3Rvir/rays_sid<SID>.csv\n",
    "    sub_<SID>/rays_and_recipes_sid<SID>_snap99_L4Rvir/rays_sid<SID>.csv\n",
    "\n",
    "REUSES (unchanged):\n",
    "- single_subhalo_rays_spectra.py  (YOUR original single-SID code;\n",
    "  do NOT edit it; this runner imports it as a module and sets globals)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import traceback\n",
    "import importlib.util\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# ========= USER PATHS (match your environment) =========\n",
    "# The repo data dir where step1 wrote the SIDs file:\n",
    "REPO_DATA = r\"../data\"\n",
    "SID_LIST  = os.path.join(REPO_DATA, \"sids_with_inc.txt\")\n",
    "\n",
    "# The parent dir that contains sub_<SID>/ folders + cutouts:\n",
    "TNG_SUBHALOS_ROOT = r\"../../../ASU Dropbox/Tanmay Singh/COS_GASS/TNG_Subhalos\"\n",
    "\n",
    "# Path to your UNCHANGED single-subhalo script:\n",
    "WORKER_SCRIPT_PATH = os.path.join(os.path.dirname(__file__), \"single_subhalo_rays_spectra.py\")\n",
    "\n",
    "# Snapshot (your single-SID script also defaults to 99)\n",
    "SNAP = 99\n",
    "\n",
    "# Which runs we *prefer* to attempt. We’ll auto-skip missing CSVs per run.\n",
    "PREFERRED_RUN_LABELS = [\"L3Rvir\", \"L4Rvir\"]\n",
    "\n",
    "# =======================================================\n",
    "\n",
    "@contextmanager\n",
    "def pushd(new_dir):\n",
    "    \"\"\"Temporarily cd into new_dir, then back.\"\"\"\n",
    "    prev = os.getcwd()\n",
    "    os.chdir(new_dir)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(prev)\n",
    "\n",
    "def debug_print_paths(sid, cutout_h5, sub_dir):\n",
    "    print(f\"[STEP2][SID={sid}] CUTOUT_H5:   {cutout_h5}\")\n",
    "    print(f\"[STEP2][SID={sid}] OUTPUT_BASE: {sub_dir}\")\n",
    "    print(f\"[STEP2][SID={sid}] CWD for worker calls will be this OUTPUT_BASE.\\n\")\n",
    "\n",
    "def find_cutout_h5_for_sid(sid: int) -> str:\n",
    "    \"\"\"\n",
    "    Try a couple patterns to locate a cutout HDF5 for this subhalo.\n",
    "    Returns first hit or None.\n",
    "    \"\"\"\n",
    "    sub_dir = os.path.join(TNG_SUBHALOS_ROOT, f\"sub_{int(sid)}\")\n",
    "    if not os.path.isdir(sub_dir):\n",
    "        return None\n",
    "    pats = [\n",
    "        os.path.join(sub_dir, f\"cutout*sub{int(sid)}*.hdf5\"),\n",
    "        os.path.join(sub_dir, \"*.hdf5\"),\n",
    "    ]\n",
    "    for pat in pats:\n",
    "        hits = sorted(glob.glob(pat))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "def rays_csv_exists(sub_dir: str, sid: int, run_label: str) -> bool:\n",
    "    rel = f\"rays_and_recipes_sid{sid}_snap{SNAP}_{run_label}/rays_sid{sid}.csv\"\n",
    "    path = os.path.join(sub_dir, rel)\n",
    "    return os.path.isfile(path)\n",
    "\n",
    "def load_worker_module():\n",
    "    if not os.path.isfile(WORKER_SCRIPT_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cannot find worker script at {WORKER_SCRIPT_PATH}\\n\"\n",
    "            f\"Please save your unmodified single-subhalo code there \"\n",
    "            f\"(filename: single_subhalo_rays_spectra.py).\"\n",
    "        )\n",
    "    spec = importlib.util.spec_from_file_location(\"single_subhalo_worker\", WORKER_SCRIPT_PATH)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "def read_sid_list(path: str):\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Missing SIDs list: {path}\\nRun step1_select_sids_with_inc.py first.\")\n",
    "    sids = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                sids.append(int(line))\n",
    "            except Exception:\n",
    "                print(\"[STEP2][WARN] Non-integer SID in list, skipping:\", line)\n",
    "    sids = sorted(set(sids))\n",
    "    print(f\"[STEP2] Loaded {len(sids)} unique SIDs from:\", path)\n",
    "    return sids\n",
    "\n",
    "def main():\n",
    "    print(\"[STEP2] Loading worker module (your single-SID script, unmodified):\")\n",
    "    worker = load_worker_module()\n",
    "    print(\"[STEP2] Worker module loaded OK:\", WORKER_SCRIPT_PATH)\n",
    "    print(\"        (We will set its globals per SID and call main())\\n\")\n",
    "\n",
    "    sids = read_sid_list(SID_LIST)\n",
    "    total = len(sids)\n",
    "    successes = 0\n",
    "    failures  = 0\n",
    "\n",
    "    for idx, sid in enumerate(sids, 1):\n",
    "        print(\"=\"*78)\n",
    "        print(f\"[STEP2] [{idx}/{total}] SID={sid}\")\n",
    "\n",
    "        sub_dir   = os.path.join(TNG_SUBHALOS_ROOT, f\"sub_{sid}\")\n",
    "        cutout_h5 = find_cutout_h5_for_sid(sid)\n",
    "\n",
    "        if not os.path.isdir(sub_dir):\n",
    "            print(f\"[STEP2][ERROR] Subhalo directory missing: {sub_dir}  -> SKIP\")\n",
    "            failures += 1\n",
    "            continue\n",
    "        if not cutout_h5 or not os.path.isfile(cutout_h5):\n",
    "            print(f\"[STEP2][ERROR] Could not find HDF5 cutout for sid={sid} under {sub_dir}  -> SKIP\")\n",
    "            failures += 1\n",
    "            continue\n",
    "\n",
    "        # Determine which runs we can actually do (rays CSV present)\n",
    "        available_runs = []\n",
    "        for run_label in PREFERRED_RUN_LABELS:\n",
    "            ok = rays_csv_exists(sub_dir, sid, run_label)\n",
    "            print(f\"[STEP2][SID={sid}] Check rays CSV for {run_label}: {'FOUND' if ok else 'MISSING'}\")\n",
    "            if ok:\n",
    "                available_runs.append(run_label)\n",
    "\n",
    "        if not available_runs:\n",
    "            print(f\"[STEP2][WARN] No rays CSV found for SID={sid} (looked for: {PREFERRED_RUN_LABELS})  -> SKIP\")\n",
    "            failures += 1\n",
    "            continue\n",
    "\n",
    "        debug_print_paths(sid, cutout_h5, sub_dir)\n",
    "\n",
    "        # Set the worker's globals for THIS SID\n",
    "        worker.CUTOUT_H5   = cutout_h5\n",
    "        worker.SID         = int(sid)\n",
    "        worker.SNAP        = int(SNAP)         # keep consistent\n",
    "        worker.OUTPUT_BASE = sub_dir\n",
    "        worker.RUN_LABELS  = available_runs    # only process runs that have rays CSV\n",
    "        # Optional: Filter by mode? (None = both)\n",
    "        # worker.FILTER_MODE = None\n",
    "\n",
    "        # Call worker.main() with CWD switched to the subhalo dir\n",
    "        try:\n",
    "            with pushd(sub_dir):\n",
    "                print(f\"[STEP2][SID={sid}] CWD now: {os.getcwd()}\")\n",
    "                print(f\"[STEP2][SID={sid}] Calling worker.main() for runs: {available_runs}\")\n",
    "                worker.main()\n",
    "            print(f\"[STEP2][SID={sid}] DONE.\")\n",
    "            successes += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[STEP2][ERROR] SID={sid} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            failures += 1\n",
    "\n",
    "    print(\"\\n\" + \"=\"*78)\n",
    "    print(f\"[STEP2] Finished. Successes={successes}  Failures={failures}  Total={total}\")\n",
    "    if failures > 0:\n",
    "        print(\"[STEP2][NOTE] See error traces above; failures do not stop the whole batch.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(\"[STEP2][FATAL]\", e)\n",
    "        traceback.print_exc()\n",
    "        sys.exit(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos-gass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
